	
	\section{Dark Silicon}
		\subsection{Obituary for Moore’s Law and Dennard Scaling}
			%REWRITE!!!!!
			In his seminal document on the subject, Gordon E. Moore predicted an exponential increase in the number of transistors in an integrated circuit [1]. As shown in Figure 1, this exponential law has become less relevant in the past 20 years, in a way that is largely explained by factors that represent the downfall of Dennard scaling [2].  \\
			The companion of Moore’s Law is Dennard Scaling. In 1974, Robert H. Dennard released a paper describing the scaling of MOSFET devices [3], a scaling which held alongside Moore’s Law until decreasing transistor size resulted in an increase in the significance of physical constraints on the scaling of circuits. In a 33 year retrospective in 2007, assumptions that underlay Dennards scaling failed. For example, Dennard assumed that there would be a continual increase in channel doping concentration that would allow continuously shorter channel lengths. However, as the doping concentration increases, impurity scattering causes degradation of carrier mobility and performance [4].
			
		\subsection{The minimum energy for computing}
			%REWRITE!!!!!
			In the last century, it was proven by Charles H Bennet and Ralph Landauer that for any irreversible computation, there is a minimum energy required required per switching equal to kBTln2, approximately 10-21J at room temperature. This is called the Shannon-von Nuemann-Landauer limit, and it describes the physical limits of switching elements, such as a CMOS transistor. This is relevant because it helps to explain the issue of dark silicon: that the transistors are not, in modern microprocessors, the primary source of energy consumption. Energy consumption in electronics is primarily caused by electrical capacitances. In modern microprocessors in the 22-65nm node range, ~20-30\% of the total energy consumed is attributed to transistors, a percentage that is expected to reduce as transistors become smaller. This is caused by an inverse relationship between leakage currents and  transistors size. The other significant losses are due to interconnects (e.g. wires) [5]. The wire interconnects can be modelled as capacitive lumps in the case of short wirse, lossy RC transmission lines the case of medium length wires, and long global wires need to be modelled as lossy RLC transmission lines. As scaling increases, these wire interconnects become the dominant limitting factor in terms of clocking frequency, delay, power and area [5, 6].
		
		\subsection{The Four Horsemen of Dark Silicon}
			%REWRITE!!!!!
			As Dennardian scaling breaks down, the period of Post-Dennardian scaling arises, shown in Table 1. There is an exponential reduction in the percentage of silicon in a chip that can switch at full frequency that decreases with every generation. An important result of this is that multicore scaling becomes prohibitive as it produces large amounts of dark silicon. Taylor, in an early survey of the dark silicon problem, defined 4 “horsemen” as solutions for the challenge of dark silicon. Ordered from most to least relevant, these are\\
			• The Specialised Horseman defined by a new age of Heterogenous core design\\
			• The Dim Horseman focusses on spreading the same power budget over more transistors\\
			• And the Shrinking Horseman, which removes dark silicon by reducing the size of the die\\
			• The Deus Ex Machina Horseman where technological advancement nullifies the issue. \\
			The Specialised Horseman relies on the principle that ASIC circuitry is usually 100 to 1000x faster than general processing units, and that general processing units typically use 157 to 707x the energy of an ASIC circuit for a similar task [7]. Heterogeneous cores and computational accelerator are both possible answers to the inefficiencies of modern CPU’s [8, 9].\\
			The Dim Horseman, focussed on dim silicon methodology, relies on underpowering or underclocking portions of the chip. Near Threshold Voltage processors and Dynamic Voltage and Frequency Scaling are both successful methods for this. Near Threshold Voltages, the voltage across the transistors is reduced to a point that is optimal for energy delay. This point is the lowest voltage that can be used without requiring a steep drop in frequency. Although intriguing, it requires additional hardware that increases the complexity of this project [10]. Whilst Dynamic Voltage Scaling is precluded for the same reason, Dynamic Frequency Scaling can be implemented with appropriate use of prescalers and will be pursued as a possible source of optimisation. \\
			Another popular method is computational sprinting, where the chip spends most of the time in a low-performance state but moved into a high-performance state for brief periods, or a second core is activated for a brief period. This brief period is a sprint and can be implemented to increase the energy efficiency as the resource of the extra CPU can be allocated as needed, and short operations can be sprinted through allowing the chip to go idle afterwards. This helps to manage the energy and thermal issues of dark silicon [11].
			The Shrinking Horseman refers to the design approach of simply designing smaller processors using less chip space to not go above the power budget on the chip.\\
			The Deus Ex Machina horseman represent new technologies, such as the QFET, the GAA transistor model, or perhaps the transistors that can be achieved through using diamond as the conducting material. Future projects on optimising a CPU scheduler for their parameters would be compelling. It stands to reason, these things being the focus of data science, that this is entirely out of scope [9].
		
		\subsection{Memory Driven Computing}
			%REWRITE!!!!!
			Memory driven computing occurs as a solution to the intense power usage of interconnects and memory blocks. In standard computing, memory is stored at varying distances and sizes with the focus being on the computing unit. In memory driven computing, the memory is the focus and the processing is on the periphery. This is an intuitive answer to the problems presented by an increased prominence of wire and memory costs. As computing move into a period where there are diminishing returns on computational performance due to externalities, switching the design focus to these externalities could prove to be a bountiful approach [12]. 
		
		\subsection{The Dark Side of Silicon}
			%REWRITE!!!!!
			In the Dark side of Silicon, Rahmani et al. describe a suite of methodologies for managing dark silicon. The heterogeneous methodology gives more direction than that of Taylor, but the section of interest is the management in the OS layer. Variation-Aware Core Selection and Scheduling is a methodology for efficient resource allocation within the power budget constraints of dark silicon [8, 13].
			The Dark Side of Silicon suggests the SiLago methodology for implementing and exploiting heterogeneity. In a problem analysis of the industry, 4 issues became apparent:\\
			• Most design focussed on the cores, ignoring the memory and interconnects\\
			• Software abstraction relies on naïve resource allocation, resulting in inefficiencies at runtime\\
			• The lack of dynamic runtime customization means that non-deterministic concurrency and communication patterns result in inefficiency power usage and parallelism\\
			•  Customisation has a large engineering cost that make it prohibitive.\\
			The SiLago platform attempts to solve these problems by implementing a Distributed Memory Architecture. This is a form of memory driven computing and has been shown to have significant reductions in power and energy expenditure if done properly [14]. Similar methodologies have found that by arbitrarily spacing memory throughout a chip, the power cost of on-chip interconnects can be severely reduced [15].
	
	\section{Task Scheduling}
	
		\subsection{Relationship with Dark Silicon}
		
		\subsection{Algorithms}
			%REWRITE!!!!!
			CPU scheduling is concerned with a number of things\\
			• CPU utilisation, keeping the CPU running as often as possible\\
			• Data throughput, a measure of CPU utilisation\\
			• Turnaround time, the period it takes for a process to be completed\\
			• Waiting time, how long the process spends waiting to use the CPU,\\
			• And response time, the time between when a process is put onto the queue and when the first response is produced\\
			Algorithms for scheduling tend to focus on the priority of tasks first, and the allocation of resources second. An example of this is a Priority Scheduler, which weighs the priority of incoming tasks and parcels out operating time based on precedence of priority. A more complex and relevant method however is a Multilevel Feedback Queue, which prevents any single process from taking up too much time [16].
		
		\subsection{Methodologies}
			
		
	
	\section{Thermal Modelling}
		\subsection{Methods}
			%REWRITE!!!!!
			Given the nature of dark silicon, it seems intuitive to use heat as a metric for progress. This approach is hobbled by the stumbling black that is the difference between the physical model of a microprocessor and an FPGA. While it is possible to gain considerable improvements to efficiency with thermal aware design designing, this increases the complexity and requires optimisation and tuning that is device specific. Gains from thermal aware design would not necessarily translate between devices, and so more translatable measures and improvements will be the focus of this project [17, 18].
		
		\subsection{Limits}
		
		\subsection{Relationship with Dark Silicon}
		
		\subsection{Impact on Chip Design}
		
	
	\section{RISC-V}
		\subsection{The RISC-V ISA}
			Placeholder for a description of the RISC-V ISA and its qualities.
			
		\subsection{RISC-V Cores}
			Placeholder for a outline of available RISC-V cores.
		
		\subsection{RISC-V Literature and Documentation}
			Placeholder for a section on the theory behind implementing a RISC-V system.
			
		\subsection{Impact on Chip Design}
		